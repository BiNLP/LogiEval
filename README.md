# LogiEval - Large Language Model Logical Reasoning Evaluation

A high-performance evaluation framework for testing large language models on logical reasoning datasets including LogiQA2.0, LogiQA, and ReClor.

## Features

- ðŸš€ **High Performance**: Optimized with vLLM for fast model inference
- ðŸ“Š **Multiple Datasets**: Support for LogiQA2.0, LogiQA, and ReClor datasets
- ðŸŽ¯ **Multiple Sampling Methods**: Direct sampling, Best-of-N (BoN), and majority voting
- ðŸ’¾ **Result Persistence**: Save intermediate results and outputs as JSON files
- ðŸ”§ **Flexible Configuration**: Easy-to-use configuration system

## Installation

```bash
pip install -r requirements.txt
```

## Quick Start

### Basic Evaluation

```bash
python evaluate.py \
    --model_name "meta-llama/Llama-2-7b-chat-hf" \
    --datasets "logiqa2" "logiqa" "reclor" \
    --sampling_method "direct" \
    --batch_size 8 \
    --output_dir "./results"
```

### Advanced Evaluation with Multiple Sampling

```bash
python evaluate.py \
    --model_name "meta-llama/Llama-2-7b-chat-hf" \
    --datasets "logiqa2" "logiqa" "reclor" \
    --sampling_method "bon" \
    --num_samples 5 \
    --batch_size 4 \
    --use_vllm \
    --output_dir "./results"
```

## Configuration

The evaluation framework supports various configuration options:

- `--model_name`: HuggingFace model identifier
- `--datasets`: List of datasets to evaluate (logiqa2, logiqa, reclor)
- `--sampling_method`: Sampling strategy (direct, bon, majority_vote)
- `--num_samples`: Number of samples for BoN and majority voting
- `--batch_size`: Batch size for inference
- `--use_vllm`: Enable vLLM acceleration
- `--temperature`: Sampling temperature
- `--max_tokens`: Maximum generation length
- `--output_dir`: Directory to save results

## Project Structure

```
LogiEval/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ logiqa2.py
â”‚   â”‚   â”œâ”€â”€ logiqa.py
â”‚   â”‚   â””â”€â”€ reclor.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ hf_model.py
â”‚   â”‚   â””â”€â”€ vllm_model.py
â”‚   â”œâ”€â”€ sampling/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ direct.py
â”‚   â”‚   â”œâ”€â”€ bon.py
â”‚   â”‚   â””â”€â”€ majority_vote.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py
â”‚       â””â”€â”€ metrics.py
â”œâ”€â”€ evaluate.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Results

Evaluation results are saved in JSON format with the following structure:

```json
{
    "config": {...},
    "results": {
        "dataset_name": {
            "accuracy": 0.85,
            "total_samples": 1000,
            "correct_predictions": 850,
            "detailed_results": [...]
        }
    },
    "intermediate_outputs": [...]
}
```
